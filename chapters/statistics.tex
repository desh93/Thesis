\chapter{Statistical analysis}\label{chap:stats}

The interpretation of measured observables, after performing a measurement, forms one of the main challenges in experimental particle physics. Statistical methods are used to quantify the relationship between some measured observables and parameters in a model. In the analysis described in this thesis, the goal is estimating the number of non-resonant signal events from the measured dataset, containing possibly both signal and background events. The estimated signal yield and the estimated background yield can be used to determine the agreement of non-resonant i.e. Contact Interactions scenarios with data, and infer values of parameters in non-resonant models. 

A central concept in statistics in particle physics is that of uncertainty, which can originate from a number of different sources. For example, uncertainties can be the result from the lack of knowledge of the experiment, or the consequence of a more fundamental random nature in the system due to quantum mechanics. A variable is said to be random when it is not known or cannot be predicted with complete certainty~\cite{Cowan1998}. The concept of probability can be used to quantify the degree of randomness. A mathematical definition of probability was formulated by Kolmogorov as set of axioms~\cite{Kol33}. 

There are two main interpretations of probability commonly used in data analysis. Most commonly used in high energy physics, the frequentist interpretation, and another interpretation known as subjective or Bayesian probability is also used. The frequentist notion of probability of an event is defined as the limit of it's relative frequency in a large number of trials. The large number of trials forms an ensemble, where in particle physics an ensemble can be formed by repeating the experiment multiple times. In frequentist probabilities, the true values of the parameters are states of nature and not the outcome of the experiment. The Bayesian definition of probability is based on a degree of belief. Therefore, Bayesian probabilities can be assigned to hypothesis on states of nature. The practical consequence of Bayesian probabilities is that prior must be supplied for various parameter values and hypothesis. 

The frequentist interpretation of probability is used for analysis in this thesis. The common use of the frequentist interpretation does not indicate an inherent merit over the Bayesian interpretation, but rather highlights the technically simple implementation of the frequentist interpretation. 

\section{Parameter estimation}
One of the most common tasks in high energy physics is estimation of some model parameter, commonly known as the parameter of interest (POI) e.g. number of signal events or signal strength in search for new physics. The POI can be estimated by measuring a set of \emph{observables}, \emph{x} who's probability depends on the POI. Depending on analysis, different quantities can be utilised as \emph{observables}, e.g. the analysis described in this thesis uses the dielectron and dimuon invariant mass. Replication of the experiment multiple times results in different values of \emph{x} resulting in a \emph{probability density function} of \emph{x}, written as $f(x;\alpha)$. The probability density function has the important property of satisfying the normalisation condition:
\begin{equation}
    \begin{aligned}
        \int_{-\infty}^{\infty} f(x;\alpha)~dx = 1
    \end{aligned}
\end{equation}

Where the parameter $\alpha$ represents some unknown parameter. $f(x)$ describes the probability density for a single event, however, when considering datasets with many events, $\mathcal{D} = (x_1,..,x_n)$, the probability density is given by the product of the densities for each event. In counting experiments, such as the one described in this analysis, the total number of selected events is also a random variable, where the total number of observed events, \emph{n} fluctuates around the expected number, $\nu$ according to a Poisson distribution. When considering binned data, common to most analyses, histograms are defined that yield a certain number of entries $\textbf{n} = (n_1,..,n_N)$ in \emph{N} bins. The expected number of events in the entries for a bin are given by:
\begin{equation}
\begin{aligned}
    \nu_i(\alpha) = n_{tot}\int_{x_i^{min}}^{x_i^{max}} f(x;\alpha)~dx
\end{aligned}
\end{equation}
where $x_{min}$ and $x_{max}$ are the bin limits. A likelihood function for the total probability distribution for \emph{N} bins can be defined:
\begin{equation}
    \label{eq:poissL}
    \begin{aligned}
        & \mathcal{L}(n;\alpha) = \prod_{i=1}^{N}\mathrm{Poisson}(n_i;\nu_i), \\
        & \mathrm{Poisson}(n;\nu) = \frac{\nu^ne^{-\nu}}{n!}.
    \end{aligned}
\end{equation}

For the single-bin counting experiment used in the analysis $N = 1$ due to only one bin used for the statistical analysis. The expected number of events in the signal region can be defined as $\nu = \nu_B +\nu_S$, where $\nu_B$ is the expected background events and $\nu_S$ is the expected signal events in signal region. The POI used in this analysis is $\nu_S$. Using the likelihood function, one can estimate the POI given the collected data. 

An \emph{estimator}, $\hat{\alpha}(\mathcal{D})$ is some function of the data and its value is used to estimate the true value of some parameter $\alpha$. There are various properties that an estimator must satisfy to be considered a \emph{good} estimator e.g bias, variance, consistency~\cite{errorsOnNumbers}. Consistency is the requirement that the estimator should converge to the true value in the limit of infinite statistics. A bias of an estimator is defined as the difference between the expectation value of the estimator $E[\hat{\alpha}]$ and the true value of $\alpha$. Variance is defined as $var[\hat{\alpha}] = E\left((\alpha - E[\hat{\alpha}])^{2}\right)$. There is a clear tradeoff between bias and variance, in the class of unbiased estimators there is a well defined minimum variance bound~\cite{Cranmer:2015nia}. When interpreting the results in the context of CI interaction models $\nu_S$ can either be expressed as a function of $\Lambda$, where it can be considered the POI. This allows for the results to be either interpreted directly in terms of lambda or the reinterpretation of the results on the number of signal events, $\nu_s$.

The most common method used estimator is the maximum likelihood estimator (MLE), defined as the value of $\alpha$ which maximises the likelihood function. In practice, due to computational efficiency, this is done by minimising $-\log\mathcal{L}$. For multi parameter likelihood functions, the various components of $\alpha_p$ are referred to as \emph{floating} parameters. The MLE method can be generalised and is used in determining, where a likelihood function can be defined and used to determine the \emph{best-fit} values for the parameters used in the background estimation. The minimisation is handled by the Minuit~\cite{James:873119} tool in the RooFit framework.

\subsubsection{Combination of channels}
Probability models can be constructed to simultaneously describe several channels defined by some associated selection criteria. The likelihood in \cref{eq:poissL} can be redefined such that a total probability function is defined for the combination of the electron and muon channels, 
\begin{equation}
    \label{eq:poissLcomb}
    \begin{aligned}
        & \mathcal{L}(n;\alpha) = \prod_{c \in channels}\prod_{i=1}^{N}\mathrm{Poisson}(n_{i,c};\nu_{i,c}), \\
    \end{aligned}
\end{equation}
where the Poisson models for the individual channels are multiplied together. For simplicity of notation the dicussion below focuses only on a single channel. However, the procedure described below is applied in the case of the combined model as well. 

\section{Treatment of systematic uncertainties}
In the example likelihood given in \cref{eq:poissL} there are additional parameters that also affect the distributions of our observables that is not the POI. These parameters are known as \emph{nuisance parameters} ($\theta$), as they must be accounted for regardless of the interest to the analysis. These include the systematic uncertainties affecting the signal and background normalisation and shape. Using the methods described in \cref{sec:extrap:uncertainties} $\pm  1\sigma$ up and down variations of $\tilde{\theta}$ are constructed. Once the uncertainties have been estimated, a constraint term $f(\theta;\tilde{\theta},\sigma_\theta)$ is multiplied by the likelihood. A Gaussian penalty term is used for the constraint term in the analysis. The Gaussian constraint is parameterised such that $\tilde{\theta} = 0$ is the nominal value of the parameter, and $\tilde{\theta} \pm 1$ are the $\pm1\sigma$ variations. The likelihood can be redefined as:
\begin{equation}
    \label{eq:likelihood}
    \begin{aligned}
        & \mathcal{L}(n;\nu,\theta) = \prod_{i=1}^{N}\mathrm{Poisson}(n_i;\nu_i(\theta)) \cdot \mathrm{Gaussian}(\tilde{\theta};\theta,\sigma_\theta), \\
        & \mathrm{Gaussian}(\tilde{\theta};\theta,\sigma_\theta) = \prod_{j=1}^{N_{syst}} \frac{1}{\sqrt{2\pi\sigma_{\theta,j}}} e^\frac{-(\tilde{\theta_j} - \theta_j)}{2\sigma_{\theta,j}^2},
    \end{aligned}
\end{equation}
where $\theta_j$ is the nuisance parameter corresponding to systematic \emph{j}. Each systematic uncertainty considered in the analysis will have a gaussian constraint on the likelihood. 

When the data is fit using the full likelihood in \cref{eq:likelihood} both the POI and nuisance parameters are estimated to minimise the likelihood. However, in some scenarios, the fit may constrain a nuisance parameter better than the POI, indicating an inherent mismodelling of the likelihood. Over constraining nuisance parameters may lead to underestimated uncertainties on the POI. The \emph{post-fit} pulls are used to quantify the degree to which the estimate of the nuisance parameter, $\hat{\theta}$ deviates from the expected value $\tilde{\theta}$ given uncertainty after fitting $\hat{\sigma}$. The pull of a nuisance parameter is defined as: 
\begin{equation}
    \label{eq:nppull}
    \begin{aligned}
        & pull = \frac{\hat{\theta} -{\theta}}{\hat{\sigma_\theta}}.
    \end{aligned}
\end{equation}
Deviation from the central value of the nuisance parameter will indicate at features of the data not described by the model. The pulls from the fit to the data are shown in \cref{chap:results}. 

\section{Hypothesis tests}
Hypothesis tests are an additional statistical tool used in the analysis to test the compatibility of the data collected with respect to two hypotheses, rejecting one over the other based on experimental observations. Two hypotheses are defined: an \emph{alternative hypothesis} ($\mathrm{H_0}$) and a \emph{null hypothesis} ($\mathrm{H_0}$). When considering an observed excess or deficit for discovery, $\mathrm{H_1}$ is defined as the expectation solely based on the SM background expectation, also referred to as the background only hypothesis. While $\mathrm{H_1}$ is defined as the expectation from SM background and a BSM process e.g. CIs, referred to as the signal+background hypothesis. When no significant excess is observed, the definitions of $\mathrm{H_1}$ and $\mathrm{H_0}$ are swapped and a potential signal can be excluded. 

In order to distinguish between the two hypothesis a \emph{test statistic}, $\mathcal{T}(\mathcal{D})$ is constructed such that the value of $\mathcal{T}$ will be different dependant on the hypothesis. An \emph{acceptance region} is defined, such that if $\mathcal{T}(\mathcal{D}) < k_\alpha$, then $H_0$ is accepted. The test statistic is defined such that it minimises the probability at which $H_0$ is rejected when it is true, known as Type-I error. In contrast, a Type-II errors describes accepting $H_0$ when $H_1$ is true, denoted by $\beta$. A test statistic is chosen such that it maximises the power of a test, $1-\beta$. There are a multitude of test statistics that can be used, however, the most power test is given by the likelihood ratio $\mathcal{T}(\mathcal{D}) = \mathcal{L}(n;\nu_{s+b})/\mathcal{L}(n;\nu_{b})$ according to the Neyman-Pearson lemma~\cite{Cowan1998}. A generalisation of the likelihood ratio known as the \emph{profile likelihood ratio} is defined as:
\begin{equation}
    \label{eq:profLL}
    \begin{aligned}
        \lambda(\nu_s) = \frac{\mathcal{L}(\nu_s;\hat{\hat{\theta}}(\nu_s))}{\mathcal{L}(\hat{\nu}_s,\hat{\theta})},
    \end{aligned}
\end{equation}
where the conditional maximum likelihood estimate $\hat{\hat{\theta(\nu_s)}}$ is the value of $\theta$ that maximises the likelihood function with $\nu_s$ fixed. The profile likelihood test statistic is used in many of the LHC experiments, including ATLAS. The test statistic is used to define the final test statistic relevant to analyses,
\begin{equation}
    \label{eq:teststat}
    \begin{aligned}
        q_{\nu_s} = -2\ln\lambda(\nu_s).
    \end{aligned}
\end{equation}

The test statistic can be used to quantify the compatibility of the observed data for a given hypothesis, known as a \emph{p}-value:
\begin{equation}
    \label{eq:pvalue}
    \begin{aligned}
        p_{\nu_s} = \int_{q_{\nu_{s},obs}}^\infty f(q_{\nu_s};\nu_s,\theta)~dq_\mu,
    \end{aligned}
\end{equation}
where $f(q_\mu;\nu_s,\theta)$ is the probability density function of the test statistic under the hypothesis $\nu_s$. $q_{\mu,obs}$ is the test statistic of the observed data. As $p_{\nu_s}$ gets smaller, the confidence that the assumed hypothesis is true decreases. 

For the purpose of discovery, the null hypothesis ($\nu_s = 0$) must be rejected. This compatibility is based on the following \emph{p}-value:
\begin{equation}
    \label{eq:pvalue_bonly}
    \begin{aligned}
        p_{0} = \int_{q_{\mu,obs}}^\infty f(q_0;0,\hat{\hat{\theta}}(\nu_s =0))~dq_\mu.
    \end{aligned}
\end{equation}
Incompatibilities with the null hypothesis at the level of $p_0 = 1.3 \times 10^{-3}$ and $p_0 = 2.9 \times 10^{-3}$ are required in order to state that observation and discovery, respectively, for new phenomena has occurred. It is customary to convert the \emph{p}-value into the sigma (quartile) of a unit Gaussian, defined as:
\begin{equation}
    \label{eq:sigma}
    \begin{aligned}
    Z = \Phi^{-1}(1-p_0),
    \end{aligned}
\end{equation}
where $\Phi^{-1}$ is the inverse of a cumulative distribution for a unit Gaussian. The significances for the observation and discovery correspond to $3\sigma$ and $5\sigma$, respectively.

When no significant excess is seem in data, a limit on the number of signal events (CI lambda scale) is set. For limit setting the test statistic $\tilde{q}_{\nu_s}$ is used to differentiate the hypothesis of the signal being produced at $\nu_s$ from the alternative hypothesis at $\nu_s^\prime < \nu_s$:
\begin{equation}
    \label{eq:limitTestStat}
    \begin{aligned}
    \tilde{q}_{\nu_s} = \begin{cases}
        -2\ln\frac{\mathcal{L}(\nu_s,\hat{\hat{\theta}}(\nu_s))}{\mathcal{L}(0,\hat{\hat{\theta}}(0))} & \mathrm{for }~\hat{\nu_s} < 0, \\
        -2\ln\frac{\mathcal{L}(\nu_s,\hat{\hat{\theta}}(\nu_s))}{\mathcal{L}(\hat{\nu}_s,\hat{\theta})} & \mathrm{for }~0 \leq \hat{\nu}_s \leq \nu_s, \\
        0          & \mathrm{for }~\hat{\nu}_s > \nu.
    \end{cases}
    \end{aligned}
\end{equation}
Using the above test statistic and \cref{eq:pvalue}, a \emph{p}-value can be defined for the hypothesis:
\begin{equation}
    \label{eq:pvalue_sb}
    \begin{aligned}
        p_{\nu_s} = \int_{\tilde{q}_{\mu,obs}}^\infty f(\tilde{q}_{\nu_s};\nu_s,\hat{\hat{\theta}}(\nu_s))~d\tilde{q}_\mu.
    \end{aligned}
\end{equation}
It was previously defined, such that if the if the test statistic is outside the acceptance region $\alpha$, the hypothesis is rejected. In this construction, the region at which $p_{\nu_s} \geq 1 - \alpha$ is referred to as the \emph{confidence level} (CL) that includes the true value of the POI with probability larger than $1-\alpha$. This would then lead to the statement that the confidence interval covers the true value of the parameter with a CL larger than $1-\alpha$. The CL value of the upper limit is commonly chosen as 95\% for ATLAS and CMS results. 

The expected limit and corresponding upper and lower bounds are calculated by sampling a distribution of the limits produced from a number of background only MC toy experiments and taking the median value. The median value is taken instead of the mean, as it is invariant to the choice of significance or \emph{p}-value. The distribution of the limit is given by $f(\nu_{s,lim};0,\hat{\hat{\theta}}(\nu_s = 0,obs))$, where the nuisance parameters have a profiled value based on the observed data. The $1\sigma$ and $2\sigma$ bands around the expected limit can be calculated by taking the 68\% and 95\% quantiles. 

\subsubsection{$\mathrm{CL}_\mathrm{s}$ construction}
Once a statement that a given hypothesis has been made by an experiment, the model being tested is no longer considered important to be searched for. Therefore, the metric by which an analysis can claim a model has been excluded has been studied in great detail. The \emph{p}-value discussed above can be subjected to statistical fluctuations and can lead to nonphysical exclusions when a downward fluctuation in the observed number of events occurs. This would result in premature exclusions of new physics models. 

Two methods where constructed to tackle this problem by the high energy physics community: the power constrained limit (PCL)~\cite{PhysRevLett.122.231801} and the $\mathrm{CL}_\mathrm{s}$ formalism~\cite{JUNK1999435,Read_2002}. The PCL method is not used widely used in the community, but represents a cutoff at a positive value of the POI everywhere. The more widely accepted, $\mathrm{CL}_\mathrm{s}$ formalism is constructed in such a way to reduce the likelihood of excluding signal hypotheses that a search is not otherwise sensitive to. The $\mathrm{CL}_\mathrm{s}$ formalism is defined as the ratio of two \emph{p}-values:
\begin{equation}
    \label{eq:cls}
    \begin{aligned}
        \mathrm{CL}_\mathrm{s} = \frac{p_{\nu_{s+b}}}{1-p_{\nu_{b}}}
    \end{aligned}
\end{equation}
where $p_{\nu_{s+b}}$ and $p_{\nu_{b}}$ quantify the compatibility between the data and the signal+background and background only hypothesis. The negative fluctuations will be moderated by the denominator, which is the same test statistic as the numerator, but with $\nu_s = 0$. $\mathrm{CL}_\mathrm{s}$ formalism is mainly used to perform hypothesis tests aimed at excluding signal models. 

\subsubsection{Asymptotic approximation}
Additionally to the toy MC approach, the confidence interval can be calculated using asymptotic formulae~\cite{Cowan:2010js}. This approximation is based on theorems by Wilks and Wald, which show that for sufficiently large data sample the distribution of the likelihood-ratio based test statistic follows a Gaussian distribution. Therefore, $-2\ln\mathcal{L}(\nu_s)$ will follow a $\chi^2$ distribution. Due the assumed distribution of the test statistic, Poisson-like distributions due to low statistics, limits obtained through the asymptotic approximation may be optimistic. Due to the single-bin signal region defined at high invariant mass where there is low statistic, the asymptotic approximation is deemed insufficient for the analysis. 

\section{Choice of parameter of interest}\label{sec:stat:poi}
The formalism above focus on the number of signal events $\nu_s$ as the parameter of interest. This allows to search for non-resonant signals and set limits in terms of the number of signal events in the signal region independent of any assumed signal models. There are two possible methods to interpret the results in terms of CI models: reinterpretation of the model independent limits in terms of CI models using the CI signal MC templates, and the other where the $\nu_s$ is parameterised in terms of the CI energy scale $\Lambda$ and limits are set on CI models directly. 

There are several steps involved with the reinterpretation of the model independent limits in the context of CI models. The CI interaction signal MC is used to map the expected number of signal events for a given model to the number of signal events. The constructive signal region results are expected to be used for constructive interference models, whereas the destructive interference signal region results will be used for destructive models. Once an interference model has been chosen, the limits on the number of signal events are mapped to the different chiral models of the CI reweighted templates described in \cref{sec:datamc:mc:sig}. A linear interpolation is performed between reweighted $\Lambda$ points to allow for signal events to be extracted between reweighted $\Lambda$ points. \cref{fig:stats:interpeeconst,fig:stats:interpeedest,fig:stats:interpmmconst,fig:stats:interpmmdest} depict the reinterpretation of the $\nu_s$ expected limits in the context of the CI $\Lambda$ limits. It's important to note that the uncertainty on the signal yield has not been taken into consideration. The procedure in including the uncertainties would involve taking the experimental uncertainty on the CI signal model and varying the expected yield by the uncertainty and calculating corresponding limit. 

Interpretation of the results directly on the CI energy scale $\Lambda$ involves parametrising the the expected signal yield, $\nu_s$ in terms of $\Lambda$. The parameterisation can be achieved by a small modification to the custom PDF class created described in \cref{sec:datamc:mc:sig:morphing},where the integral of the expected number of signal events in the SR can be provided as a function of $\Lambda$. This allows the POI to be defined as $\Lambda$. The expected number of events predicted in the signal yield can then be written as $\nu_{s+b} = \nu_b +\nu_s(\Lambda)$. An advantage of using $\Lambda$ as the POI is that is allows for a common parameter between the electron and muon channels. This allows the likelihood to be defined for the combination of the electron and muon channels in terms of $\Lambda$. The limits set on the number of events for the combined channel has no obvious mapping to $\Lambda$. This is due to different signal yields can be predicted in the electron and muon channels for a given $\Lambda$ caused by the different detector efficiencies of electrons and muons. 

The expected limits in the electron and muon channels are compared for the two choices of POI in \cref{tab:limits_comparePOI}. The expected limits shown indicate that there is a little difference between interpreting the limits directly on $\Lambda$ and reinterpreting them from the limits on the number of signal events. Due the added benefit of having limits on the combined electron and muon channels, the results are interpreted directly in terms of $\Lambda$ in addition to the model independent limits produced on the number of signal events. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-LL.pdf}
        \label{fig:bkgmodel:interpee1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-LR.pdf}
        \label{fig:bkgmodel:interpee2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-RL.pdf}
        \label{fig:bkgmodel:interpee3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-RR.pdf}
        \label{fig:bkgmodel:interpee4}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to it's $\Lambda$ value. The constructive interference signal regions are shown for the electron channel.}
    \label{fig:stats:interpeeconst}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-LL.pdf}
        \label{fig:bkgmodel:interpee5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-LR.pdf}
        \label{fig:bkgmodel:interpee6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-RL.pdf}
        \label{fig:bkgmodel:interpee7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-RR.pdf}
        \label{fig:bkgmodel:interpee8}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to it's $\Lambda$ value. The destructive interference signal regions are shown for the electron channel.}
    \label{fig:stats:interpeedest}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-LL.pdf}
        \label{fig:bkgmodel:interpmm1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-LR.pdf}
        \label{fig:bkgmodel:interpmm2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-RL.pdf}
        \label{fig:bkgmodel:interpmm3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-RR.pdf}
        \label{fig:bkgmodel:interpmm4}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to it's $\Lambda$ value. The constructive interference signal regions are shown for the muon channel.}
    \label{fig:stats:interpmmconst}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-LL.pdf}
        \label{fig:bkgmodel:interpmm5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-LR.pdf}
        \label{fig:bkgmodel:interpmm6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-RL.pdf}
        \label{fig:bkgmodel:interpmm7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-RR.pdf}
        \label{fig:bkgmodel:interpmm8}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to it's $\Lambda$ value. The destructive interference signal regions are shown for the muon channel.}
    \label{fig:stats:interpmmdest}
\end{figure}

\begin{table}[htp]
    \begin{center}  
{\begin{tabular}{r c c c c c c c c c c c}\toprule
    & & & \multicolumn{4}{c}{Expected limit [TeV]} \\
    Int. & Channel & POI & LL & LR & RL & RR \\
    \midrule
    \multirow{3}{*}[-1em]{\begin{sideways}Constructive\end{sideways}} & \multirow{2}{*}{$ee$} & $\Lambda$ & 31.1 & 28.9 & 28.7 & 30.9 \\
    & & $\nu_s$ & 31.1 & 28.8 & 28.7 & 30.9 \\
    \cmidrule{2-7}
     & \multirow{2}{*}{$\mu\mu$} & $\Lambda$ & 29.6 & 27.4 & 27.3 & 29.5 \\
    & & $\nu_s$ & 29.8 & 27.6 & 27.5 & 29.6 \\
    \midrule
    \multirow{3}{*}[-1em]{\begin{sideways}Destructive\end{sideways}} & \multirow{2}{*}{$ee$} & $\Lambda$ & 23.0 & 24.4 & 24.4 & 23.2 \\
    & & $\nu_s$ & 23.0 & 24.4 & 24.4 & 23.2 \\
    \cmidrule{2-7}
     & \multirow{2}{*}{$\mu\mu$} & $\Lambda$ & 22.3 & 23.9 & 23.9 & 22.8 \\
    & & $\nu_s$ & 22.6 & 24.1 & 24.1 & 22.8 \\
\bottomrule\end{tabular}}
\caption{Comparison of expected limits from reinterpreting the results in terms of $\Lambda$ from limits on the number of signal events ($\nu_s$) and limits set directly on $\Lambda$ as the parameter of interest.}  
\label{tab:limits_comparePOI}
\end{center}
\end{table}

\clearpage
