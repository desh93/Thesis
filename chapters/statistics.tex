\chapter{Overview of statistical concepts}\label{chap:stats}
The following chapter provides an overview of the concepts and procedure used in the statistical analysis performed on the analysis presented in this thesis. An overview of the interpretation of probability is given in \cref{sec:stats:interp}, where the interpretation used in the analysis is also provided. The construction of the likelihood used in the analysis and the method used to estimate its associated parameters are provided in \cref{sec:stats:param}. \cref{sec:stats:nps} describes the treatment of the systematic uncertainties considered in the analysis. \cref{sec:stats:hypo} describes the procedure of testing the compatibility of the data with respect to a hypothesis in the context of potential discovery, or setting a limit. Finally, \cref{sec:stat:poi} discusses the two possible methods available for the interpretation of results in the context of CI models, where the final method used will be outlined.

\section{Interpretation of probability}\label{sec:stats:interp}
The interpretation of measured observables after performing a measurement forms one of the main challenges in experimental particle physics. Statistical methods are used to quantify the relationship between some measured observables and parameters in a model. In the analysis presented in this thesis, the goal is testing different non-resonant signal models by measuring the number of events with different values of dilepton mass. The estimated signal yield and the estimated background yield can be used to determine the agreement of non-resonant, i.e. Contact Interactions scenarios (\cref{chap:bsm}), with data, and infer values of parameters in non-resonant models. 
  
A central concept in statistics in particle physics is that of uncertainty, which can originate from several different sources. For example, uncertainties can be the result of the lack of knowledge of the experiment, or the consequence of a more fundamental random nature in the system due to quantum mechanics. A variable is said to be random when it is not known or cannot be predicted with complete certainty~\cite{Cowan1998}. The concept of probability can be used to quantify the degree of randomness. Kolmogorov formulated a mathematical definition of probability as a set of axioms~\cite{Kol33}. 

The interpretation of probability can take two forms. The frequentist interpretation, and another known as subjective or Bayesian probability is also used. The frequentist notion of probability of an event is defined as the limit of its relative frequency in a large number of trials. A large number of trials forms an ensemble, where in particle physics an ensemble can be formed by repeating the experiment multiple times. In frequentist probabilities, the true values of the parameters are states of nature and not the outcome of the experiment. The Bayesian definition of probability is based on a degree of belief. Therefore, Bayesian probabilities can be assigned to hypothesis on states of nature. The practical consequence of Bayesian probabilities is that for various parameters a prior probability must be supplied, which describes one's belief about its true value. 

The frequentist interpretation of probability is used for analysis in this thesis. The frequentist vs. Bayesian choice is an ongoing debate in the community and the high-energy physics community mainly uses frequentist methods as they do not require specific prior degrees of belief. The use of the frequentist interpretation does not indicate an inherent merit over the Bayesian interpretation, but rather highlights the technically simple implementation of the frequentist interpretation.

\section{Parameter estimation}\label{sec:stats:param}
One of the most common tasks in high energy physics is an estimation of some model parameter~\cite{Cowan1998}, commonly known as the parameter of interest (POI), e.g. a number of signal events or signal strength in search for new physics. Additional parameters that also affect the model that are not the POI are known as nuisance parameters, this is described in detail in \cref{sec:stats:nps}. The POI can be estimated by measuring a set of \emph{observables}, \emph{x}, whose probability depends on the POI. Depending on the analysis, different quantities can be utilised as \emph{observables}, e.g. the analysis described in this thesis uses the dielectron and dimuon invariant mass. Replication of the experiment multiple times results in different values of \emph{x}, which results in a \emph{probability density function} of \emph{x}, written as $f(x;\alpha)$, where $\alpha$ is used to represent any parameters associated with the physical theory or an unknown property associated with the detector response. 

When considering a dataset with many events, $\mathcal{D} = (x_1,..,x_n)$, the probability density is given by the product of the densities for each event. In counting experiments, such as the one described in this analysis, the total number of selected events is also a random variable, where the total number of observed events, \emph{n}, fluctuates around the expected number of events, $\nu$, according to a Poisson distribution. When considering binned data, common to most analyses, histograms are defined that yield a certain number of entries $n = (n_1,..,n_N)$ in \emph{N} bins. The expected number of events in the entries for a bin is given by
\begin{equation}
\begin{aligned}
    \nu(\alpha) = n_{\mathrm{tot}}\int_{x_{i,\mathrm{min}}}^{x_{i,\mathrm{max}}} f(x;\alpha)~dx,
\end{aligned}
\end{equation}
where $x_{i,\mathrm{min}}$ and $x_{i,\mathrm{max}}$ are the bin limits, and $n_{\mathrm{tot}}$ is the total number of events in the dataset. The expected number of events are estimated for each of the signal regions (SRs) defined in the analysis, where the bin limits corresponds to the start and end of the signal region as defined in \cref{chap:bkgmodel}. A likelihood function for the total probability distribution can be defined
\begin{equation}
    \label{eq:poissL}
    \begin{aligned}
        & \mathcal{L}(n;\alpha) = \frac{\nu^{n}e^{-\nu}}{n!}. \\
    \end{aligned}
\end{equation}

For the single-bin counting experiment used in the analysis $N = 1$ due to only one bin used for the statistical analysis. The expected number of events in the bin can be defined as $\nu = \nu_S + \nu_B$, where $\nu_B$ is the SM expected background events, and $\nu_S$ is the signal events in the bin. The POI used in this analysis is $\nu_S$. Using the likelihood function, one can estimate the POI given the collected data. 

An \emph{estimator}, $\hat{\alpha}(\mathcal{D})$ is some function of the data and its value is used to estimate the true value of some parameter $\alpha$. There are various properties that an estimator must satisfy to be considered a \emph{good} estimator, e.g. bias, variance, consistency~\cite{errorsOnNumbers}. Consistency is the requirement that the estimator should converge to the true value in the limit of infinite statistics. A bias of an estimator is defined as the difference between the expectation value of the estimator, $E[\hat{\alpha}]$, and the true value of $\alpha$. Variance is defined as $var[\hat{\alpha}] = E\left[(\alpha - E[\hat{\alpha}])^{2}\right]$. There is a clear trade-off between bias and variance, in the class of unbiased estimators, there is a well-defined minimum variance bound~\cite{Cranmer:2015nia}. When interpreting the results in the context of CI interaction models, $S$ can be expressed as a function of CI energy scale $\Lambda$, as describe in \cref{chap:bsm}, where $\Lambda$ can also be considered the POI. This allows for the results to be either interpreted directly in terms of lambda or the reinterpretation of the results from  the number of signal events, $\nu_S$.

The most commonly used estimator is the maximum likelihood estimator (MLE), defined as the value of $\alpha$, which maximises the likelihood function. In practice, due to computational efficiency, this is done by minimising $-\log\mathcal{L}$. For multi-parameter likelihood functions, the various components of $\alpha_p$ are referred to as \emph{floating} parameters. The MLE method can be generalised and is used to determine the \emph{best-fit} values for the parameters used in the background estimation. The minimisation is handled by the Minuit~\cite{James:873119} tool in the RooFit framework.

\subsubsection{Combination of channels}
Probability models can be constructed to describe several channels defined by some associated selection criteria simultaneously. The likelihood in \cref{eq:poissL} can be redefined for the combination of electron and muon channels 
\begin{equation}
    \label{eq:poissLcomb}
    \begin{aligned}
        & \mathcal{L}(n_c;\alpha) = \prod_{\mathrm{c}~\in~\mathrm{channels}}~\frac{\nu_{c}^{n_{c}} e^{-\nu_{c}}}{n_{c}!}, \\
    \end{aligned}
\end{equation}
where $c$ represents the channels being considered and $\alpha$ are the parameters associated with the model. For simplicity of notation, the discussion below focuses only on a single channel. However, the procedure described below is applied to the combined model as well. 

\section{Treatment of systematic uncertainties}\label{sec:stats:nps}
In the example likelihood given in \cref{eq:poissL}, there are additional parameters that also affect the distributions of the observables that are not the POI. These parameters are known as \emph{nuisance parameters} ($\theta$), since they must be accounted for regardless of the interest to the analysis. These are related to systematic uncertainties affecting the signal and background normalisation and shape. Using the methods described in \cref{chap:uncertBkgmodel}, $\pm 1\sigma$ up and down variations of ${\theta}$ are constructed. Once the uncertainties have been estimated, a constraint term $f(\tilde{\theta};\theta,\sigma_\theta)$ is multiplied by the likelihood. A Gaussian penalty term is used for the constraint term in the analysis. The Gaussian constraint is parameterised such that $\tilde{\theta} = 0$ is the nominal value of the parameter, and $\tilde{\theta} \pm 1$ are the $\pm1\sigma$ variations. The $\tilde{\theta}$ values are treated on the same footing as the measurement, and they are independent from other parts of the data. Therefore, the joint probability of $n$ and $\tilde{\theta}$ is given by the product of their probabilities. The likelihood can be redefined as
\begin{equation}
    \label{eq:likelihood}
    \begin{aligned}
        & \mathcal{L}(n;\nu_S,\theta) = \frac{\nu^{n} e^{-\nu}}{n!} \times \prod_{j=1}^{N} \frac{1}{\sqrt{2\pi}\sigma_{j}} e^\frac{-(\tilde{\theta}_j - \theta_j)}{2\sigma_{j}^2} , \\
    \end{aligned}
\end{equation}
where $\theta_j$ is the nuisance parameter corresponding to systematic \emph{j}, $\sigma_j$ are the $1\sigma$ variations and the expected number of events is a function of the POI and the nuisance parameters, $\nu = \nu_S + \nu_B(\theta)$.

When the data are fit using the full likelihood in \cref{eq:likelihood}, both the POI and nuisance parameters are estimated to minimise the likelihood. However, in some scenarios, standard deviation of the nuisance parameter after the fit can be substantially smaller than what entered into the constraint term, indicating an inherent mismodelling of the likelihood. The \emph{post-fit} pulls are used to quantify the degree to which the estimate of the nuisance parameter, $\hat{\theta}$, deviates from the nominal value, $\tilde{\theta}$, given the uncertainty after fitting, $\hat{\sigma}_{\hat{\theta}}$. The pull of a nuisance parameter is defined as
\begin{equation}
    \label{eq:nppull}
    \begin{aligned}
        & \mathrm{pull} = \frac{\hat{\theta} -\tilde{\theta}}{\hat{\sigma}_{\hat{\theta}}}.
    \end{aligned}
\end{equation}
Deviation from the central value of the nuisance parameter will indicate features of the data not described by the model. The pulls of the nuisance parameters from the fit to data are shown in \cref{chap:results}. 

The impact of the nuisance parameter on the parameter of interest is defined as
\begin{equation}
    \label{eq:npinpact}
    \begin{aligned}
        & \mathrm{Impact} = \Delta \hat{\nu}_{S} = \hat{\hat{\nu}}_S(\theta \pm \sigma_\theta) - \hat{\nu}_S,
    \end{aligned}
\end{equation}
where, $\hat{\hat{\nu}}_S(\theta \pm \sigma_\theta)$, is the fitted value of $\nu_S$ with the nuisance parameter fixed at its nominal value ($\tilde{\theta}$) plus or minus its standard deviation, resulting in the \emph{prefit} impact. The postfit impact is given by the nuisance parameter fixed at its estimated value ($\hat{\theta}$) plus or minus its standard deviation. 


\section{Hypothesis tests}\label{sec:stats:hypo}
Hypothesis tests are an additional statistical tool used in the analysis to test the compatibility of the data collected with respect to two hypotheses, rejecting one over the other based on experimental observations. Two hypotheses are defined: an \emph{alternative hypothesis} ($\mathrm{H_1}$) and a \emph{null hypothesis} ($\mathrm{H_0}$), where $\mathrm{H}_0$ is synonymous with the hypothesis that will be tested. When considering an observed excess of events for a potential discovery, $\mathrm{H_0}$ is defined as the expectation solely based on the SM background expectation, also referred to as the background-only hypothesis. whilst $\mathrm{H_1}$ is defined as the expectation from SM background and a BSM process, e.g. CIs, is referred to as the signal+background hypothesis. The claim of discovery is that the data are incompatible with the background-only hypothesis. When no significant excess is observed, the incompatibility of the data with signal+background hypothesis is tested, and a potential signal can be excluded. 

In order to distinguish between the two hypothesis a \emph{test statistic}, $\mathcal{T}(\mathcal{D})$ is constructed such that the value of $\mathcal{T}$ will be different dependant on the hypothesis. An \emph{acceptance region} is defined, such that if $\mathcal{T}(\mathcal{D}) < k_\alpha$, then $H_0$ is accepted. The test statistic is defined such that it minimises the probability at which $H_0$ is rejected when it is true, known as a Type-I error. In contrast, a Type-II errors describes accepting $H_0$ when $H_1$ is true, denoted by $\beta$. A test statistic is chosen such that it maximises the power of a test, $1-\beta$, by fixing size of the test  ($\alpha$). There are a multitude of test statistics that can be used. The most powerful test with respect to a single alternative hypothesis is given by the likelihood ratio $\mathcal{T}(\mathcal{D}) = \mathcal{L}(n;\nu_{S+B})/\mathcal{L}(n;\nu_{B})$ according to the Neyman-Pearson lemma~\cite{Cowan1998}. A generalisation of the likelihood ratio known as the \emph{profile likelihood ratio} is defined as
\begin{equation}
    \label{eq:profLL}
    \begin{aligned}
        \lambda(\nu_S) = \frac{\mathcal{L}(\nu_S;\hat{\hat{\theta}}(\nu_S))}{\mathcal{L}(\hat{\nu}_S,\hat{\theta})},
    \end{aligned}
\end{equation}
where the \emph{conditional maximum likelihood estimate}, $\hat{\hat{\theta}}(\nu_S)$,  is the value of $\theta$ that maximises the likelihood function with $\nu_S$ fixed. The profile likelihood test statistic is used in many of the LHC experiments, including ATLAS. The test statistic is used to define the final test statistic relevant to analyses
\begin{equation}
    \label{eq:teststat}
    \begin{aligned}
        q_{\nu_S} = -2\ln\lambda(\nu_S),
    \end{aligned}
\end{equation}

where it is used to quantify the compatibility of the observed data for a given hypothesis. The probability to have obtained data with a certain property assuming a hypothesis~\cite{Cranmer:2015nia} is given by the \emph{p}-value, 
\begin{equation}
    \label{eq:pvalue}
    \begin{aligned}
        p_{\nu_S} = \int_{q_{\nu_{s},obs}}^\infty f(q_{\nu_S};\nu_S,\theta)~dq_{\nu_S},
    \end{aligned}
\end{equation}
where $f(q_{\nu_S};\nu_S,\theta)$ is the probability density function of the test statistic under the hypothesis $\nu_S$, and $q_{\nu_S,obs}$ is the test statistic of the observed data. As $p_{\nu_S}$ gets smaller, the confidence that the assumed hypothesis is true decreases. 

For discovery, the null hypothesis ($\nu_S = 0$) must be rejected. This compatibility is based on the following \emph{p}-value
\begin{equation}
    \label{eq:pvalue_bonly}
    \begin{aligned}
        p_{0} = \int_{q_{\nu_S,obs}}^\infty f(q_0;0,\hat{\hat{\theta}}(\nu_S =0))~dq_{\nu_S}.
    \end{aligned}
\end{equation}
It is customary to convert the \emph{p}-value into the sigma of a standard Gaussian, defined as
\begin{equation}
    \label{eq:sigma}
    \begin{aligned}
    Z = \Phi^{-1}(1-p_0),
    \end{aligned}
\end{equation}
where $\Phi^{-1}$ is the inverse of a cumulative distribution for a standard Gaussian. The significances for evidence or a discovery correspond to $3\sigma$ and $5\sigma$, respectively. These correspond to values of $p_0 = 1.3 \times 10^{-3}$ and $p_0 = 2.9 \times 10^{-7}$ for $3\sigma$ and $5\sigma$, respectively.

When no significant excess is observed in data, a limit on the number of signal events or CI $\Lambda$ scale is set. For limit setting the test statistic, $\tilde{q}_{\nu_S}$, is used to test the hypothesis of the signal being produced at $\nu_S$ from the alternative hypothesis at $\nu_S^\prime < \nu_S$~\cite{Cowan:2010js},
\begin{equation}
    \label{eq:limitTestStat}
    \begin{aligned}
    \tilde{q}_{\nu_S} = \begin{cases}
        -2\ln\frac{\mathcal{L}(\nu_S,\hat{\hat{\theta}}(\nu_S))}{\mathcal{L}(0,\hat{\hat{\theta}}(0))} & \mathrm{for }~\hat{\nu_S} < 0, \\
        -2\ln\frac{\mathcal{L}(\nu_S,\hat{\hat{\theta}}(\nu_S))}{\mathcal{L}(\hat{\nu}_s,\hat{\theta})} & \mathrm{for }~0 \leq \hat{\nu}_s \leq \nu_S, \\
        0          & \mathrm{for }~\hat{\nu}_s > \nu.
    \end{cases}
    \end{aligned}
\end{equation}
A \emph{p}-value can be defined for the hypothesis using the above test statistic and \cref{eq:pvalue}, 
\begin{equation}
    \label{eq:pvalue_sb}
    \begin{aligned}
        p_{\nu_S} = \int_{\tilde{q}_{\nu_S,obs}}^\infty f(\tilde{q}_{\nu_S};\nu_S,\hat{\hat{\theta}}(\nu_S))~d\tilde{q}_{\nu_S}.
    \end{aligned}
\end{equation}
It was previously defined if the test statistic is outside the acceptance region $\alpha$, the hypothesis is rejected. Using this definition, $p_{\nu_S} \geq \alpha$ is referred to as the \emph{confidence level} (CL) that includes the true value of the POI with probability larger than $1-\alpha$. This definition of CL would then lead to the statement that the confidence interval covers the true value of the parameter with a CL larger than $1-\alpha$. The CL value of the upper limit is commonly chosen as 95\% for ATLAS and CMS results. Signal hypothesis will be tested until the point at which the signal hypothesis is found that is no longer excluded at the 95\% confidence interval. 

\subsubsection{$\mathrm{CL}_\mathrm{s}$ construction}
Once a statement on the exclusion of a model has been made, the model is no longer considered essential to be searched. Therefore, the metric by which analyses can claim a model has been excluded has been studied in great detail. The \emph{p}-value discussed above can be subjected to statistical fluctuations and can lead to non-physical exclusions when a downward fluctuation in the observed number of events occurs. This would result in premature exclusions of new physics models. 

Two methods are constructed to tackle this problem by the high-energy particle physics community: the power-constrained limit (PCL)~\cite{PhysRevLett.122.231801} and the $\mathrm{CL}_\mathrm{s}$ formalism~\cite{JUNK1999435,Read_2002}. The PCL method is not used widely used in the community but represents a cutoff at a positive value of the POI everywhere. The more widely accepted, $\mathrm{CL}_\mathrm{s}$ formalism is constructed to reduce the likelihood of excluding signal hypotheses that a search is not otherwise sensitive. The $\mathrm{CL}_\mathrm{s}$ formalism is defined as the ratio of two \emph{p}-values:
\begin{equation}
    \label{eq:cls}
    \begin{aligned}
        \mathrm{CL}_\mathrm{s} = \frac{p_{\nu_{S}}}{1-p_{\nu_{B}}}
    \end{aligned}
\end{equation}
where $p_{\nu_{B}}$ is the \emph{p}-value derived from the same test statistic under the background-only hypothesis. The negative fluctuations will be moderated by the denominator, which is the same test statistic as the numerator, but with $\nu_S = 0$. The $\mathrm{CL}_\mathrm{s}$ formalism is mainly used to perform hypothesis tests aimed at excluding signal models. 

\subsubsection{Expected limit}
The expected limit is an important quantity which indicates the sensitivity of an analysis. The expected limit and corresponding upper and lower bounds are calculated by sampling a distribution of the limits produced from many background-only MC toy experiments and taking the median value of the expected limit distribution, and its 95\% and 68\% quantiles. The median value is taken instead of the mean, as it is invariant to the choice of significance or \emph{p}-value. The distribution of the limit is given by $f(\nu_{S,\mathrm{up}};0,\hat{\hat{\theta}}(\nu_S = 0,obs))$, where the nuisance parameters have a profiled value based on the observed data. The $1\sigma$ and $2\sigma$ bounds indicate the rage of fluctuations that one expects for the limit under the assumption of the background-only model. 

\cref{fig:toyDist} depicts the distribution of limits from toy MC experiments in the constructive and destructive SRs in the electron and muon channel. The results are based on 2000 toy experiments. The 68\% and 95\% quantiles of the distribution corresponding to the one and two sigma error bands, respectively, are indicated on the distributions by the yellow and green bands. The Non-Gaussian behaviour observed in the distributions leads to an asymmetry in the $1\sigma$ and $2\sigma$ bands. This is due to the distribution of the test statistic becoming discrete when there are low number of events in the signal regions; this is especially prominent in the muon destructive signal regions and can be seen in the final results in \cref{chap:results}. 

\begin{figure}[!htpb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/toyLimitDist-ee-const.pdf}
        \label{fig:toyDist1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/toyLimitDist-mm-dest.pdf}
        \label{fig:toyDist2}
    \end{subfigure}
    \caption[Expected limit distribution for 2000 toy MC experiments.]{Expected limit distribution for 2000 toy MC experiments in the electron constructive (left) and muon destructive (right) signal region for the LL CI model. The dashed line indicates the median of the distribution. The green and yellow bands correspond to the 68\% and 95\% quantiles, respectively, of the distribution.}
    \label{fig:toyDist}
\end{figure}

\subsubsection{Asymptotic approximation}
In addition to the MC approach described above, the confidence interval can be calculated using asymptotic formulae~\cite{Cowan:2010js}. This approximation is based on theorems by Wilks and Wald~\cite{wilks1938}, which show that for sufficiently large data sample, the distribution of the likelihood-ratio based test statistic follows a Gaussian distribution. Therefore, $-2\ln\mathcal{L}(\nu_S)$ will follow a $\chi^2$ distribution. The probability $p_0$ can be directly calculated from $q_0$ as~\cite{Cowan:2010js}
\begin{equation}
    \label{eq:p0_asymp}
    \begin{aligned}
        p_0 = 1 - \Phi\left(\sqrt{q_0}\right),
    \end{aligned}
\end{equation}
and the significance takes the form $Z = \sqrt{q_0}$. The asymptotic approach considerably reduces the computing time and resources required. 

Due to the assumed distribution of the test statistic, for Poisson-like distributions with low statistics limits obtained using the asymptotic approximation may overestimate the limit. The single-bin signal regions defined at high invariant mass for the analysis have a small number of events. Therefore, the asymptotic approximation is deemed insufficient for the analysis. A comparison of the expected limits from the MC method and the asymptotic approximation is shown in \cref{fig:asympVstoy}. The results from the asymptotic approach predict an expected limit that is optimistic compared to the toy approach. 

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.7\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/toyLimitVsAsymp-ee-const.pdf}
    \label{fig:asympVstoy1}
    \caption[Comparison of expected limit from the toy MC method and the asymptotic approximation.]{Comparison of expected limit from the toy MC method and the asymptotic approximation in constructive electron signal region for the LL chiral CI model. The expected limit from the toy MC method and the asymptotic approximation is shown by the red and black dashed lines, respectively.}
    \label{fig:asympVstoy}
\end{figure}

\section{Choice of parameter of interest}\label{sec:stat:poi}
The formalism above focus on the number of signal events $\nu_S$ as the parameter of interest. This choice of POI allows to search for non-resonant signals and set limits in terms of the number of signal events in the signal region. There are two possible methods to interpret the results in terms of CI models: a reinterpretation of the model-independent limits in terms of CI models using the CI signal MC, and the other where the $\nu_S$ is parameterised in terms of the CI energy scale $\Lambda$ and limits are set on CI models directly.  

There are several steps involved with the reinterpretation of the model-independent limits on $\nu_S$ in the context of CI models. The CI interaction signal MC is used to map the expected number of signal events for a given model to the number of signal events. The constructive signal region results are expected to be used for constructive interference CI models, whereas the destructive interference signal region results are used for destructive models. Once an interference model has been chosen, the limits on the number of signal events are mapped to the different chiral models of the CI reweighted templates described in \cref{sec:datamc:mc:sig}. A linear interpolation is performed between reweighted $\Lambda$ points to allow for signal events to be extracted between reweighted $\Lambda$ points. \cref{fig:stats:interpeeconst,fig:stats:interpeedest,fig:stats:interpmmconst,fig:stats:interpmmdest} depict the reinterpretation of the $\nu_S$ expected limits in the context of the CI $\Lambda$ limits. It is important to note that the uncertainty on the signal yield has not been taken into consideration. The procedure in including the uncertainties would involve taking the experimental uncertainty on the CI signal model and varying the expected yield by the uncertainty and calculating the corresponding limit. 

Interpretation of the results directly on the CI energy scale $\Lambda$ involves parametrising the expected signal yield, $\nu_S$ in terms of $\Lambda$. The parameterisation can be achieved by a small modification to the custom PDF class created described in \cref{sec:datamc:mc:sig:morphing}, where the integral of the expected number of signal events in the SR is provided as a function of $\Lambda$. This allows the POI to be defined as $\Lambda$. The expected number of events predicted in the signal yield can then be written as $\nu = \nu_B +\nu_S(\Lambda)$. An advantage of using $\Lambda$ as the POI is that it uses a common parameter between the electron and muon channels. This allows the likelihood to be defined for the combination of the electron and muon channels in terms of $\Lambda$. The limits set on the number of events for the combined channel has no trivial mapping to $\Lambda$, due to the different detector efficiencies in the electron and muon channels.  

The expected limits in the electron and muon channels are compared for the two choices of POI in \cref{tab:limits_comparePOI}. The limits shown indicate that there is a little difference between interpreting the limits directly on $\Lambda$ and reinterpreting them from the limits on the number of signal events. However, due to the added benefit of being able to easily provide limits on the combined electron and muon channels, the final results on the CI models are interpreted directly in terms of $\Lambda$. Nevertheless, the model-independent limits on $\nu_S$ are still presented to facilitate reinterpretation into other non-resonant BSM models. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-LL.pdf}
        \label{fig:bkgmodel:interpee1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-LR.pdf}
        \label{fig:bkgmodel:interpee2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-RL.pdf}
        \label{fig:bkgmodel:interpee3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-const-RR.pdf}
        \label{fig:bkgmodel:interpee4}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to its $\Lambda$ value. The constructive interference signal regions are shown for the electron channel.}
    \label{fig:stats:interpeeconst}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-LL.pdf}
        \label{fig:bkgmodel:interpee5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-LR.pdf}
        \label{fig:bkgmodel:interpee6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-RL.pdf}
        \label{fig:bkgmodel:interpee7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-ee-dest-RR.pdf}
        \label{fig:bkgmodel:interpee8}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to its $\Lambda$ value. The destructive interference signal regions are shown for the electron channel.}
    \label{fig:stats:interpeedest}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-LL.pdf}
        \label{fig:bkgmodel:interpmm1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-LR.pdf}
        \label{fig:bkgmodel:interpmm2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-RL.pdf}
        \label{fig:bkgmodel:interpmm3}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-const-RR.pdf}
        \label{fig:bkgmodel:interpmm4}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to its $\Lambda$ value. The constructive interference signal regions are shown for the muon channel.}
    \label{fig:stats:interpmmconst}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-LL.pdf}
        \label{fig:bkgmodel:interpmm5}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-LR.pdf}
        \label{fig:bkgmodel:interpmm6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-RL.pdf}
        \label{fig:bkgmodel:interpmm7}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{/Users/Deshan/Documents/PhD/thesis/Thesis/figures/analysis/stats/intersection-mm-dest-RR.pdf}
        \label{fig:bkgmodel:interpmm8}
    \end{subfigure}
    \caption{Distribution of the expected yield in the signal region corresponding to $\Lambda$ values. The expected limit on the number of signal events is depicted and the mapping to its $\Lambda$ value. The destructive interference signal regions are shown for the muon channel.}
    \label{fig:stats:interpmmdest}
\end{figure}

\begin{table}[htp]
    \begin{center}  
{\begin{tabular}{r c c c c c c c c c c c}\toprule
    & & & \multicolumn{4}{c}{Expected limit [TeV]} \\
    Int. & Channel & POI & LL & LR & RL & RR \\
    \midrule
    \multirow{3}{*}[-1em]{\begin{sideways}Constructive\end{sideways}} & \multirow{2}{*}{$\ee$} & $\Lambda$ & 31.1 & 28.9 & 28.7 & 30.9 \\
    & & $\nu_S$ & 31.1 & 28.8 & 28.7 & 30.9 \\
    \cmidrule{2-7}
     & \multirow{2}{*}{$\mumu$} & $\Lambda$ & 29.8 & 27.6 & 27.5 & 29.6 \\
    & & $\nu_S$ & 29.8 & 27.6 & 27.5 & 29.6 \\
    \midrule
    \multirow{3}{*}[-1em]{\begin{sideways}Destructive\end{sideways}} & \multirow{2}{*}{$\ee$} & $\Lambda$ & 23.0 & 24.4 & 24.4 & 23.2 \\
    & & $\nu_S$ & 23.0 & 24.4 & 24.4 & 23.2 \\
    \cmidrule{2-7}
     & \multirow{2}{*}{$\mumu$} & $\Lambda$ & 22.6 & 24.0 & 24.2 & 22.8 \\
    & & $\nu_S$ & 22.6 & 24.1 & 24.1 & 22.8 \\
\bottomrule\end{tabular}}
\caption{Comparison of expected limits from reinterpreting the results in terms of $\Lambda$ from limits on the number of signal events ($\nu_S$) and limits set directly on $\Lambda$ as the parameter of interest. The limits are shown for the various CI chiral and interference models in the electron and muon channels.}  
\label{tab:limits_comparePOI}
\end{center}
\end{table}

\clearpage
